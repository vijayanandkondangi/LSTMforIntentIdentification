{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UimumJofoPr3"
   },
   "source": [
    "# LSTM Based Bi-directional RNN for Intent Identification from Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M4jhe2wYbisO",
    "outputId": "4694c603-7e74-4329-82ac-8903d12c7fd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "# Import the required libraries\n",
    "\n",
    "%tensorflow_version 2.x\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import InputLayer\n",
    "from keras.optimizers import SGD,Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TgzykZUtcatr",
    "outputId": "2e51cad3-1b52-4d67-91b9-2431bc6b4d26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# Mount the google drive folder and load the CSV files in to pandas dataframes\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive', force_remount = True)\n",
    "os.chdir('/content/gdrive/My Drive/IntentClassification')\n",
    "\n",
    "intents = pd.read_csv(\"intents.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "XP9sF3-yciuf",
    "outputId": "4aafbb21-f678-44fa-8140-5fb80ab29a63"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserEntry</th>\n",
       "      <th>Intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how to create order</td>\n",
       "      <td>HOWTO_CREATE_ORDER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how to log order</td>\n",
       "      <td>HOWTO_CREATE_ORDER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>list of steps to create order</td>\n",
       "      <td>HOWTO_CREATE_ORDER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>list of steps to log order</td>\n",
       "      <td>HOWTO_CREATE_ORDER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what are the steps to create order</td>\n",
       "      <td>HOWTO_CREATE_ORDER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            UserEntry              Intent\n",
       "0                 how to create order  HOWTO_CREATE_ORDER\n",
       "1                    how to log order  HOWTO_CREATE_ORDER\n",
       "2       list of steps to create order  HOWTO_CREATE_ORDER\n",
       "3          list of steps to log order  HOWTO_CREATE_ORDER\n",
       "4  what are the steps to create order  HOWTO_CREATE_ORDER"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the content of the dataframe\n",
    "\n",
    "intents.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "dpY9rIWIxpcv"
   },
   "outputs": [],
   "source": [
    "# Read and process glove vectors\n",
    "\n",
    "# Open the glove vectors text file in read mode\n",
    "f = open(\"glove.6B.50d.txt\",'r')\n",
    "\n",
    "# Initialize index variable to zero\n",
    "ind = 0\n",
    "\n",
    "# Initialize dictionaries to store the representations\n",
    "word_to_index = {}\n",
    "index_to_word = {}\n",
    "word_to_vec_map = {}\n",
    "\n",
    "# For loop to iterate through the lines read from the file\n",
    "for line in f:\n",
    "  # Split the line read, to read the words and vector representations\n",
    "  splitLines = line.split()\n",
    "  # Store the word in a variable\n",
    "  word = splitLines[0]\n",
    "  # Store the word vectors in a numpy array\n",
    "  wordEmbedding = np.array([float(value) for value in splitLines[1:]])\n",
    "\n",
    "  # Write to the dictionaries\n",
    "  word_to_index[word] = ind\n",
    "  index_to_word[ind] = word\n",
    "  word_to_vec_map[word] = wordEmbedding\n",
    "\n",
    "  # Increment the index\n",
    "  ind = ind + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_juoeX4k8NMk",
    "outputId": "1029ebac-5f0f-442e-8d96-3163b37ecfaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Length of word to index dict is :  400000\n",
      "Length of index to word dict is :  400000\n",
      "Length of word to vector dict is:  400000\n",
      "\n",
      "\n",
      "Index of happy is:  1751\n",
      "Word at index 25 is:  from\n",
      "\n",
      "\n",
      "Word vector for happy is:\n",
      "  [ 0.092086  0.2571   -0.58693  -0.37029   1.0828   -0.55466  -0.78142\n",
      "  0.58696  -0.58714   0.46318  -0.11267   0.2606   -0.26928  -0.072466\n",
      "  1.247     0.30571   0.56731   0.30509  -0.050312 -0.64443  -0.54513\n",
      "  0.86429   0.20914   0.56334   1.1228   -1.0516   -0.78105   0.29656\n",
      "  0.7261   -0.61392   2.4225    1.0142   -0.17753   0.4147   -0.12966\n",
      " -0.47064   0.3807    0.16309  -0.323    -0.77899  -0.42473  -0.30826\n",
      " -0.42242   0.055069  0.38267   0.037415 -0.4302   -0.39442   0.10511\n",
      "  0.87286 ]\n"
     ]
    }
   ],
   "source": [
    "# Examine the W2V dictionaries returned from the previous step\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Length of word to index dict is : \", len(word_to_index))\n",
    "print(\"Length of index to word dict is : \", len(index_to_word))\n",
    "print(\"Length of word to vector dict is: \",len(word_to_vec_map))\n",
    "print(\"\\n\")\n",
    "print(\"Index of happy is: \", word_to_index[\"happy\"])\n",
    "print(\"Word at index 25 is: \", index_to_word[25])\n",
    "print(\"\\n\")\n",
    "print(\"Word vector for happy is:\\n \", word_to_vec_map[\"happy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "dsf3a-9o-PgE"
   },
   "outputs": [],
   "source": [
    "# Function to convert an array of sentences to W2V indices after preprocessing\n",
    "\n",
    "def sentences_to_indices(X, word_to_index, max_len = 40):\n",
    "\n",
    "  # Identify No. of sentences in the input\n",
    "  m = len(X)\n",
    "  # Initialize the indices array to zeros based on the dimensions\n",
    "  X_indices = np.zeros((m, max_len))\n",
    "\n",
    "  # For loop to iterate through the elements of the array one by one\n",
    "  for i in range(m):\n",
    "\n",
    "    # Convert to lower case\n",
    "    X[i] = str(X[i]).lower()\n",
    "    # Remove punctuations\n",
    "    X[i] = re.sub(r'[^\\w\\s]', '', X[i]) \n",
    "    # Remove new line\n",
    "    X[i] = re.sub(r'[\\n]', ' ', X[i])\n",
    "    # Remove underscore\n",
    "    X[i] = re.sub(r'[_]', '', X[i])\n",
    "\n",
    "    # Split the sentence to words\n",
    "    sentence_words = X[i].lower().split()\n",
    "\n",
    "    # Initialize the word counter to zero\n",
    "    j = 0\n",
    "    # For loop to iterate through the words in each of the sentences\n",
    "    for w in sentence_words:\n",
    "      # Update the indices array with the index of the word at the required position\n",
    "      # If a word does not exist in the array, treat it as unknown ('unk')\n",
    "      if (w in word_to_index):\n",
    "        X_indices[i, j] = word_to_index[w]\n",
    "      else:\n",
    "        X_indices[i, j] = word_to_index['unk']\n",
    "      j = j + 1\n",
    "  \n",
    "  # Return the indices array\n",
    "  return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Wv4RrECW_yPz"
   },
   "outputs": [],
   "source": [
    "# Function to create the pretrained embedding layer for Keras\n",
    "\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "\n",
    "  # Initialize the vocabulary length\n",
    "  vocab_len = len(word_to_index) + 1\n",
    "  # Extract max vector dimension\n",
    "  emb_dim = word_to_vec_map[\"happy\"].shape[0]\n",
    "  # Initialize a numpy array with zeros for the embedding layer\n",
    "  emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "\n",
    "  # Iterate through every word and get the vector representation\n",
    "  for word, idx in word_to_index.items():\n",
    "    emb_matrix[idx, :] = word_to_vec_map[word]\n",
    "\n",
    "  # Create the Keras embedding layer\n",
    "  embedding_layer = Embedding(vocab_len, emb_dim)\n",
    "  # Make the embedding layer non-trainable\n",
    "  embedding_layer.build((None,))\n",
    "\n",
    "  # Set the weights based on the vectors read previously\n",
    "  embedding_layer.set_weights([emb_matrix])\n",
    "\n",
    "  # Return the embedding layer\n",
    "  return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KGXA7sZkA-c2",
    "outputId": "a3ffd295-8705-4b38-d4d1-a959dd91d58a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 40)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 40, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 40, 256)           183296    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 40, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 1028      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 20,578,614\n",
      "Trainable params: 20,578,614\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compose the Bidirectional LSTM based model\n",
    "\n",
    "# Compose the Input layer\n",
    "# Set max length based on the value determined previously\n",
    "sentence_indices = Input(shape = 40, dtype = \"int32\")\n",
    "\n",
    "# Fetch the word embeddings as the embedding layer\n",
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "\n",
    "# Set the embedding layer as part of the model\n",
    "embeddings = embedding_layer(sentence_indices)\n",
    "\n",
    "# Initialize max classes count\n",
    "max_class_cnt = 4\n",
    "\n",
    "# Layer 1\n",
    "X = Bidirectional(LSTM(units = 128, return_sequences = True))(embeddings)\n",
    "# Dropout\n",
    "X = Dropout(0.5)(X)\n",
    "# Layer 2\n",
    "X = Bidirectional(LSTM(units = 128, return_sequences = False))(X)\n",
    "# Dropout\n",
    "X = Dropout(0.5)(X)\n",
    "\n",
    "# Output layer with softmax activation\n",
    "X = Dense(units = max_class_cnt)(X)\n",
    "X = Activation('softmax')(X)\n",
    "\n",
    "# Compose model\n",
    "model = Model(inputs = sentence_indices, outputs = X)\n",
    "\n",
    "# Compile and summarize the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "CmGqJzzgd5jC"
   },
   "outputs": [],
   "source": [
    "# Compose the concatenated user entry pairs for training\n",
    "\n",
    "userentry = intents[\"UserEntry\"].tolist()\n",
    "classes = intents[\"Intent\"].tolist()\n",
    "\n",
    "# Convert the train sentences in to indices\n",
    "X_train_indices = sentences_to_indices(userentry, word_to_index, 40)\n",
    "\n",
    "# Initialize the onehot encoder frok sklearn\n",
    "enc = OneHotEncoder(sparse = False)\n",
    "enc.fit(np.array(classes).reshape(-1, 1))\n",
    "\n",
    "# Convert the train labels to one hot vectors\n",
    "y_train_oh = enc.transform(np.array(classes).reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hcgnPa3PMT4H",
    "outputId": "a952381e-7ab1-42f3-fa16-a2fd0531950a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "20/20 [==============================] - 9s 216ms/step - loss: 1.4463 - accuracy: 0.3042\n",
      "Epoch 2/20\n",
      "20/20 [==============================] - 4s 212ms/step - loss: 1.2804 - accuracy: 0.4740\n",
      "Epoch 3/20\n",
      "20/20 [==============================] - 4s 215ms/step - loss: 1.5054 - accuracy: 0.3431\n",
      "Epoch 4/20\n",
      "20/20 [==============================] - 4s 215ms/step - loss: 1.2608 - accuracy: 0.4983\n",
      "Epoch 5/20\n",
      "20/20 [==============================] - 4s 216ms/step - loss: 1.2954 - accuracy: 0.4848\n",
      "Epoch 6/20\n",
      "20/20 [==============================] - 4s 216ms/step - loss: 1.1251 - accuracy: 0.4744\n",
      "Epoch 7/20\n",
      "20/20 [==============================] - 4s 215ms/step - loss: 1.0129 - accuracy: 0.5486\n",
      "Epoch 8/20\n",
      "20/20 [==============================] - 4s 216ms/step - loss: 0.6180 - accuracy: 0.7059\n",
      "Epoch 9/20\n",
      "20/20 [==============================] - 4s 216ms/step - loss: 0.5974 - accuracy: 0.7482\n",
      "Epoch 10/20\n",
      "20/20 [==============================] - 4s 212ms/step - loss: 0.4850 - accuracy: 0.7333\n",
      "Epoch 11/20\n",
      "20/20 [==============================] - 4s 215ms/step - loss: 0.1540 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "20/20 [==============================] - 4s 216ms/step - loss: 0.0441 - accuracy: 0.9964\n",
      "Epoch 13/20\n",
      "20/20 [==============================] - 4s 218ms/step - loss: 0.3588 - accuracy: 0.8729\n",
      "Epoch 14/20\n",
      "20/20 [==============================] - 4s 216ms/step - loss: 0.1581 - accuracy: 0.9198\n",
      "Epoch 15/20\n",
      "20/20 [==============================] - 4s 215ms/step - loss: 0.0893 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "20/20 [==============================] - 4s 213ms/step - loss: 0.0171 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "20/20 [==============================] - 4s 213ms/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "20/20 [==============================] - 4s 211ms/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "20/20 [==============================] - 4s 215ms/step - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "20/20 [==============================] - 4s 217ms/step - loss: 0.0054 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "\n",
    "history = model.fit(X_train_indices, y_train_oh, epochs = 20, batch_size = 2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "q5dP-CMAKiUP"
   },
   "outputs": [],
   "source": [
    "# Save the model as a h5 file\n",
    "\n",
    "model.save(\"rnn_biLSTM_intent_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1bn-PT_Qx6RK",
    "outputId": "496322f7-e533-473e-eb23-cfb6fb8f3f37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entered Text:  what are the steps to be followed to create an SR?\n",
      "\n",
      "Max Probability Intent:  HOWTO_CREATE_SR\n",
      "\n",
      "Probability Scores...\n",
      "\n",
      " Intent                         \t Probability\n",
      "\n",
      " HOWTO_CREATE_ORDER             \t 0.016069567\n",
      "\n",
      " HOWTO_CREATE_SR                \t 0.9801159\n",
      "\n",
      " HOWTO_UPDATE_ORDER             \t 0.0013079132\n",
      "\n",
      " HOWTO_UPDATE_SR                \t 0.0025066053\n"
     ]
    }
   ],
   "source": [
    "# Enter test sentence\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "X_test = \"what are the steps to be followed to create an SR?\"\n",
    "#------------------------------------------------------------------------\n",
    "\n",
    "# Convert user text to indices\n",
    "X_test_list=[]\n",
    "X_test_list.append(X_test)\n",
    "X_test_indices = sentences_to_indices(X_test_list, word_to_index, 40)\n",
    "\n",
    "# Make prediction on the user entered text\n",
    "pred = model.predict(X_test_indices)\n",
    "\n",
    "# Print the outputs\n",
    "print(\"\\nEntered Text: \", X_test)\n",
    "print(\"\\nMax Probability Intent: \", enc.categories_[0][np.argmax(pred[0])])\n",
    "\n",
    "print(\"\\nProbability Scores...\")\n",
    "\n",
    "print(\"\\n\", \"Intent\".ljust(30, ' '), \"\\t\", \"Probability\")\n",
    "for i in range(len(pred[0])):\n",
    "  print(\"\\n\", enc.categories_[0][i].ljust(30, ' '), \"\\t\", pred[0][i])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "LSTM Based RNN for Intent Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
